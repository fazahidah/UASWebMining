{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "Data preprocessing adalah proses yang mengubah data mentah ke dalam bentuk yang lebih mudah dipahami. Proses ini penting dilakukan karena data mentah sering kali tidak memiliki format yang teratur. Selain itu, data mining juga tidak dapat memproses data mentah, sehingga proses ini sangat penting dilakukan untuk mempermudah proses berikutnya, yakni analisis data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c020d3",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6669bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re #regrex libray\n",
    "import nltk\n",
    "import swifter\n",
    "import Sastrawi\n",
    "import networkx as nx\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205dda63",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c0ef2b6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             abstrak\n0                                      ABSTRAK\\r\\...\n1                                        ABSTRACT...\n2  Electronic journals, also known as ejournals, ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abstrak</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABSTRAK\\r\\...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABSTRACT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Electronic journals, also known as ejournals, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "data_berita = pd.read_csv('ptamanajemen.csv')\n",
    "data_berita.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7a628",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "449b1906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Case Folding Result : \n\n0                                        abstrak\\r\\...\n1                                          abstract...\n2    electronic journals, also known as ejournals, ...\nName: abstrak, dtype: object\n\n\n\n\n"
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "data_berita['abstrak'] = data_berita['abstrak'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(data_berita['abstrak'].head(20))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728553b",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b5894ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokenizing Result : \n\n0    [abstrak, satiyah, pengaruh, faktorfaktor, pel...\n1    [abstract, in, an, effort, to, increase, labor...\n2    [electronic, journals, also, known, as, ejourn...\nName: abstrak, dtype: object\n\n\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Fatin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(data_berita['abstrak'].head(20))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5036a5b",
   "metadata": {},
   "source": [
    "## Menghitung Frekuensi Distribusi Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2318a10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Frequency Tokens : \n\n0    [(dan, 20), (dengan, 13), (produktivitas, 9), ...\n1    [(the, 25), (of, 21), (and, 18), (to, 12), (pr...\n2    [(are, 4), (they, 4), (electronic, 3), (journa...\nName: abstrak, dtype: object\n"
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(data_berita['abstrak'].head(20).apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77b94c",
   "metadata": {},
   "source": [
    "## Filtering (Stopword Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5354eade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    [produktivitas, kerja, pelatihan, pengembangan...\n1    [the, of, and, to, productivity, is, training,...\n2    [are, they, electronic, journals, ejournals, t...\nName: abstrak, dtype: object\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Fatin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"ptamanajemen.csv\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(data_berita['abstrak'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a9cdb",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ee05c81",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    [produktivitas, kerja, pelatihan, pengembangan...\n1    [the, of, and, to, productivity, is, training,...\n2    [are, they, electronic, journals, ejournals, t...\nName: abstrak, dtype: object"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "normalizad_word = pd.read_csv(\"ptamanajemen.csv\")\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[0] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].apply(normalized_term)\n",
    "\n",
    "data_berita['abstrak'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb23492",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad76fc38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "292\n------------------------\n\n\nPandas Apply:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A{'produktivitas': 'produktivitas', 'kerja': 'kerja', 'pelatihan': 'latih', 'pengembangan': 'kembang', 'pegawai': 'pegawai', 'dinas': 'dinas', 'kelautan': 'laut', 'perikanan': 'ikan', 'bangkalan': 'bangkal', 'pengaruh': 'pengaruh', 'faktorfaktor': 'faktorfaktor', 'penelitian': 'teliti', 'meningkat': 'tingkat', 'instansi': 'instansi', 'hubungan': 'hubung', 'metode': 'metode', 'seleksi': 'seleksi', 'variabel': 'variabel', 'responden': 'responden', 'dianalisis': 'analis', 'sampling': 'sampling', 'analisis': 'analisis', 'jabatan': 'jabat', 'motivasi': 'motivasi', 'peserta': 'serta', 'kabupaten': 'kabupaten', 'terbukti': 'bukti', 'nilai': 'nilai', 'satiyah': 'satiyah', 'dibawah': 'bawah', 'bimbingan': 'bimbing', 'drahjsanugrahini': 'drahjsanugrahini', 'irawatimm': 'irawatimm', 'helmi': 'helm', 'buyung': 'buyung', 'auliasstsemmt': 'auliasstsemmt', 'upaya': 'upaya', 'meningkatkan': 'tingkat', 'mudah': 'mudah', 'salah': 'salah', 'usaha': 'usaha', 'menerapkan': 'terap', 'program': 'program', 'sumber': 'sumber', 'daya': 'daya', 'manusia': 'manusia', 'sdm': 'sdm', 'dilaksanakan': 'laksana', 'tercapai': 'capai', 'kemampuan': 'mampu', 'efektif': 'efektif', 'efisien': 'efisien', 'pengembnagan': 'pengembnagan', 'diharapkan': 'harap', 'menyesuaikan': 'sesuai', 'kebutuhankebutuhan': 'kebutuhankebutuhan', 'sikap': 'sikap', 'tingkah': 'tingkah', 'laku': 'laku', 'keterampilan': 'terampil', 'pengetahuan': 'tahu', 'sesuai': 'sesuai', 'tuntutan': 'tuntut', 'perubahan': 'ubah', 'mendukung': 'dukung', 'terciptanya': 'cipta', 'suasana': 'suasana', 'kondusif': 'kondusif', 'produktivitasi': 'produktivitas', 'tujuan': 'tuju', 'mengukur': 'ukur', 'menganalisa': 'menganalisa', 'peneliti': 'teliti', 'pendekatan': 'dekat', 'observasional': 'observasional', 'analitik': 'analitik', 'pengamatan': 'amat', 'langsung': 'langsung', 'penyebaran': 'sebar', 'kuisioner': 'kuisioner', 'populasi': 'populasi', 'sampel': 'sampel', 'diolah': 'olah', 'spss': 'spss', 'versi': 'versi', 'statistik': 'statistik', 'non': 'non', 'probality': 'probality', 'simple': 'simple', 'random': 'random', 'kesimpulan': 'simpul', 'perbedaan': 'beda', 'individu': 'individu', 'partisipasi': 'partisipasi', 'aktif': 'aktif', 'instruktur': 'instruktur', 'simultan': 'simultan', 'koefisien': 'koefisien', 'determinasi': 'determinasi', 'berganda': 'ganda', 'square': 'square', 'fhitung': 'fhitung', 'ftabel': 'ftabel', 'faktor': 'faktor', 'berpengaruh': 'pengaruh', 'parsial': 'parsial', 'pengujian': 'uji', 'hipotesis': 'hipotesis', 'thitung': 'thitung', 'dominan': 'dominan', 'kunci': 'kunci', 'the': 'the', 'of': 'of', 'and': 'and', 'to': 'to', 'productivity': 'productivity', 'is': 'is', 'training': 'training', 'development': 'development', 'employees': 'employees', 'with': 'with', 'in': 'in', 'agency': 'agency', 'marine': 'marine', 'fisheries': 'fisheries', 'that': 'that', 'working': 'working', 'study': 'study', 'increase': 'increase', 'be': 'be', 'by': 'by', 'work': 'work', 'this': 'this', 'factors': 'factors', 'relationship': 'relationship', 'analysis': 'analysis', 'selection': 'selection', 'on': 'on', 'an': 'an', 'effort': 'effort', 'not': 'not', 'can': 'can', 'which': 'which', 'much': 'much', 'influence': 'influence', 'department': 'department', 'namely': 'namely', 'respondents': 'respondents', 'methods': 'methods', 'motivation': 'motivation', 'participants': 'participants', 'effect': 'effect', 'as': 'as', 'abstract': 'abstract', 'labor': 'labor', 'easy': 'easy', 'because': 'because', 'its': 'its', 'one': 'one', 'implement': 'implement', 'programs': 'programs', 'human': 'human', 'resource': 'resource', 'need': 'need', 'implemented': 'implemented', 'so': 'so', 'high': 'high', 'achieved': 'achieved', 'increasing': 'increasing', 'ability': 'ability', 'effectively': 'effectively', 'efficiently': 'efficiently', 'also': 'also', 'expected': 'expected', 'able': 'able', 'adjust': 'adjust', 'new': 'new', 'needs': 'needs', 'attitude': 'attitude', 'behavior': 'behavior', 'skills': 'skills', 'knowledge': 'knowledge', 'accordance': 'accordance', 'changing': 'changing', 'demands': 'demands', 'good': 'good', 'support': 'support', 'creation': 'creation', 'conducive': 'conducive', 'atmosphere': 'atmosphere', 'institutions': 'institutions', 'self': 'self', 'will': 'will', 'purpose': 'purpose', 'know': 'know', 'how': 'how', 'against': 'against', 'employment': 'employment', 'measure': 'measure', 'analyze': 'analyze', 'between': 'between', 'two': 'two', 'variables': 'variables', 'above': 'above', 'researchers': 'researchers', 'used': 'used', 'observational': 'observational', 'analytic': 'analytic', 'approach': 'approach', 'make': 'make', 'observations': 'observations', 'directly': 'directly', 'spread': 'spread', 'questionnaire': 'questionnaire', 'for': 'for', 'population': 'population', 'sample': 'sample', 'processed': 'processed', 'version': 'version', 'analyzed': 'analyzed', 'using': 'using', 'statistical': 'statistical', 'method': 'method', 'conclusions': 'conclusions', 'are': 'are', 'consists': 'consists', 'individual': 'individual', 'differences': 'differences', 'position': 'position', 'active': 'active', 'participation': 'participation', 'instructors': 'instructors', 'have': 'have', 'simultaneous': 'simultaneous', 'evidenced': 'evidenced', 'value': 'value', 'coefficient': 'coefficient', 'multiple': 'multiple', 'determination': 'determination', 'or': 'or', 'fstatistic': 'fstatistic', 'ftable': 'ftable', 'greater': 'greater', 'than': 'than', 'factor': 'factor', 'term': 'term', 'partial': 'partial', 'no': 'no', 'testing': 'testing', 'second': 'second', 'hypothesis': 'hypothesis', 'suggests': 'suggests', 'variable': 'variable', 'proven': 'proven', 'seen': 'seen', 'from': 'from', 'tcount': 'tcount', 'indicates': 'indicates', 'has': 'has', 'dominant': 'dominant', 'at': 'at', 'keywords': 'keywords', 'they': 'they', 'electronic': 'electronic', 'journals': 'journals', 'ejournals': 'ejournals', 'known': 'known', 'scholarly': 'scholarly', 'intellectual': 'intellectual', 'magazines': 'magazines', 'accessed': 'accessed', 'via': 'via', 'transmission': 'transmission', 'practice': 'practice', 'means': 'means', 'usually': 'usually', 'published': 'published', 'web': 'web', 'specialized': 'specialized', 'form': 'form', 'document': 'document', 'providing': 'providing', 'material': 'material', 'academic': 'academic', 'research': 'research', 'formatted': 'formatted', 'approximately': 'approximately', 'like': 'like', 'journal': 'journal', 'articles': 'articles', 'traditional': 'traditional', 'printed': 'printed'}\n------------------------\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute '_is_builtin_func'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\swifter\\swifter.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr_logging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                 \u001b[0mtmp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m                 \u001b[0msample_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12600/141925463.py\u001b[0m in \u001b[0;36mget_stemmed_term\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_stemmed_term\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mterm_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12600/141925463.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_stemmed_term\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mterm_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12600/141925463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mterm_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mdata_berita\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstrak'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_berita\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstrak'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_stemmed_term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_berita\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstrak'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\swifter\\swifter.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_progress_bar_desc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"Pandas Apply\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m                     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5573\u001b[0m         ):\n\u001b[0;32m   5574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute '_is_builtin_func'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data_berita['abstrak']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    i=0\n",
    "    if i<10:\n",
    "        term_dict[term] = stemmed_wrapper(term)\n",
    "        #print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data_berita['abstrak'] = data_berita['abstrak'].swifter.apply(get_stemmed_term)\n",
    "print(data_berita['abstrak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9dac3",
   "metadata": {},
   "source": [
    "## Simpan Data ke CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e80f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berita.to_csv(\"TextPreprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf12b8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c1802b8",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             abstrak\n0  ['produktivitas', 'kerja', 'pelatihan', 'penge...\n1  ['the', 'of', 'and', 'to', 'productivity', 'is...\n2  ['are', 'they', 'electronic', 'journals', 'ejo...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abstrak</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['produktivitas', 'kerja', 'pelatihan', 'penge...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['the', 'of', 'and', 'to', 'productivity', 'is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['are', 'they', 'electronic', 'journals', 'ejo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data_berita = pd.read_csv(\"TextPreprocessing.csv\", usecols=[\"abstrak\"])\n",
    "data_berita.columns = [\"abstrak\"]\n",
    "\n",
    "data_berita.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73971e0e",
   "metadata": {},
   "source": [
    "## Term Frekuensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ccdbe00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocabulary:  {'produktivitas': 208, 'kerja': 127, 'pelatihan': 185, 'pengembangan': 191, 'pegawai': 184, 'dinas': 59, 'kelautan': 125, 'perikanan': 197, 'bangkalan': 31, 'pengaruh': 190, 'faktorfaktor': 82, 'penelitian': 188, 'meningkat': 153, 'instansi': 112, 'hubungan': 100, 'metode': 158, 'seleksi': 234, 'variabel': 280, 'responden': 222, 'dianalisis': 54, 'sampling': 227, 'analisis': 15, 'jabatan': 120, 'motivasi': 159, 'peserta': 199, 'kabupaten': 123, 'terbukti': 255, 'nilai': 168, 'satiyah': 228, 'dibawah': 55, 'bimbingan': 38, 'drahjsanugrahini': 65, 'irawatimm': 117, 'helmi': 96, 'buyung': 39, 'auliasstsemmt': 30, 'upaya': 274, 'meningkatkan': 154, 'mudah': 162, 'salah': 224, 'usaha': 275, 'menerapkan': 150, 'program': 210, 'sumber': 252, 'daya': 48, 'manusia': 144, 'sdm': 230, 'dilaksanakan': 58, 'tercapai': 256, 'kemampuan': 126, 'efektif': 67, 'efisien': 72, 'pengembnagan': 192, 'diharapkan': 57, 'menyesuaikan': 155, 'kebutuhankebutuhan': 124, 'sikap': 237, 'tingkah': 266, 'laku': 139, 'keterampilan': 129, 'pengetahuan': 193, 'sesuai': 236, 'tuntutan': 272, 'perubahan': 198, 'mendukung': 149, 'terciptanya': 257, 'suasana': 250, 'kondusif': 135, 'produktivitasi': 209, 'tujuan': 271, 'mengukur': 152, 'menganalisa': 151, 'peneliti': 187, 'pendekatan': 186, 'observasional': 172, 'analitik': 16, 'pengamatan': 189, 'langsung': 140, 'penyebaran': 195, 'kuisioner': 136, 'populasi': 200, 'sampel': 225, 'diolah': 60, 'spss': 245, 'versi': 283, 'statistik': 248, 'non': 170, 'probality': 205, 'simple': 238, 'random': 217, 'kesimpulan': 128, 'perbedaan': 196, 'individu': 109, 'partisipasi': 183, 'aktif': 12, 'instruktur': 115, 'simultan': 239, 'koefisien': 134, 'determinasi': 51, 'berganda': 35, 'square': 246, 'fhitung': 83, 'ftabel': 90, 'faktor': 81, 'berpengaruh': 36, 'parsial': 179, 'pengujian': 194, 'hipotesis': 98, 'thitung': 265, 'dominan': 63, 'kunci': 137, 'the': 262, 'of': 175, 'and': 21, 'to': 267, 'productivity': 207, 'is': 118, 'training': 269, 'development': 53, 'employees': 75, 'with': 289, 'in': 105, 'agency': 11, 'marine': 145, 'fisheries': 84, 'that': 261, 'working': 291, 'study': 249, 'increase': 106, 'be': 32, 'by': 40, 'work': 290, 'this': 264, 'factors': 80, 'relationship': 218, 'analysis': 17, 'selection': 233, 'on': 176, 'an': 14, 'effort': 71, 'not': 171, 'can': 41, 'which': 287, 'much': 161, 'influence': 111, 'department': 50, 'namely': 164, 'respondents': 223, 'methods': 157, 'motivation': 160, 'participants': 181, 'effect': 68, 'as': 26, 'abstract': 3, 'labor': 138, 'easy': 66, 'because': 33, 'its': 119, 'one': 177, 'implement': 103, 'programs': 211, 'human': 101, 'resource': 221, 'need': 165, 'implemented': 104, 'so': 242, 'high': 97, 'achieved': 7, 'increasing': 107, 'ability': 0, 'effectively': 69, 'efficiently': 70, 'also': 13, 'expected': 78, 'able': 1, 'adjust': 9, 'new': 167, 'needs': 166, 'attitude': 29, 'behavior': 34, 'skills': 241, 'knowledge': 132, 'accordance': 6, 'changing': 42, 'demands': 49, 'good': 92, 'support': 253, 'creation': 47, 'conducive': 45, 'atmosphere': 28, 'institutions': 113, 'self': 235, 'will': 288, 'purpose': 215, 'know': 131, 'how': 99, 'against': 10, 'employment': 76, 'measure': 148, 'analyze': 19, 'between': 37, 'two': 273, 'variables': 282, 'above': 2, 'researchers': 220, 'used': 276, 'observational': 173, 'analytic': 18, 'approach': 22, 'make': 143, 'observations': 174, 'directly': 61, 'spread': 244, 'questionnaire': 216, 'for': 85, 'population': 201, 'sample': 226, 'processed': 206, 'version': 284, 'analyzed': 20, 'using': 277, 'statistical': 247, 'method': 156, 'conclusions': 44, 'are': 24, 'consists': 46, 'individual': 110, 'differences': 56, 'position': 202, 'active': 8, 'participation': 182, 'instructors': 114, 'have': 95, 'simultaneous': 240, 'evidenced': 77, 'value': 279, 'coefficient': 43, 'multiple': 163, 'determination': 52, 'or': 178, 'fstatistic': 89, 'ftable': 91, 'greater': 93, 'than': 260, 'factor': 79, 'term': 258, 'partial': 180, 'no': 169, 'testing': 259, 'second': 231, 'hypothesis': 102, 'suggests': 251, 'variable': 281, 'proven': 212, 'seen': 232, 'from': 88, 'tcount': 254, 'indicates': 108, 'has': 94, 'dominant': 64, 'at': 27, 'keywords': 130, 'they': 263, 'electronic': 74, 'journals': 122, 'ejournals': 73, 'known': 133, 'scholarly': 229, 'intellectual': 116, 'magazines': 142, 'accessed': 5, 'via': 285, 'transmission': 270, 'practice': 203, 'means': 147, 'usually': 278, 'published': 214, 'web': 286, 'specialized': 243, 'form': 86, 'document': 62, 'providing': 213, 'material': 146, 'academic': 4, 'research': 219, 'formatted': 87, 'approximately': 23, 'like': 141, 'journal': 121, 'articles': 25, 'traditional': 268, 'printed': 204}\nEncoded Document is:\n[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1\n  1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0\n  1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n  0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0\n  1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1\n  0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n  0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0\n  1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0\n  0 0 0 0]\n [1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0\n  0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n  0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1\n  1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n  0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0\n  1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1\n  1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1\n  0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1\n  1 1 1 1]\n [0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0\n  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n  0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n  0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0\n  0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1\n  0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n  0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0\n  0 0 0 0]]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a=len(document)\n",
    "document = data_berita['abstrak']\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(document)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7abf5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac3629",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c37e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "tf = tfidf.fit_transform(vectorizer.fit_transform(document)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23e72752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a])\n",
    "# dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b60c23",
   "metadata": {},
   "source": [
    "## Convert File to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58075c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.to_csv(\"TF-IDF.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}