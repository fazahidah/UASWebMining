{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9341bf79ac503efb2b0c19cae84b17b48c30e211"
   },
   "source": [
    "# Modelling dengan LSA\n",
    "\n",
    "Latent Semantic Analysis (LSA) merupakan sebuah metode yang memanfaatkan model statistik matematis untuk menganalisa struktur semantik suatu teks. LSA bisa digunakan untuk menilai esai dengan mengkonversikan esai menjadi matriks-matriks yang diberi nilai pada masing-masing term untuk dicari kesamaan dengan term referensi. Secara umum, langkah-langkah LSA dalam penilaian esai adalah sebagai berikut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a503c2851673a3d3ec1eb2c3d6c6e5566292af1"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "_uuid": "39f140e85d319161012fb67f258dd6b339545b0a",
    "id": "5O5_-2kTGD6V"
   },
   "outputs": [],
   "source": [
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2b520342f1761de78285e76aa6327d9d37b1f4e"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "_uuid": "aad495ed355e99947bc3b9816cfad0ff430c17e5",
    "id": "xaPWCpthHVIS"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TextPreprocessing.csv\", usecols=[\"deskripsi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "_uuid": "e09dd69ffa2bae1920f3e10b1af0a8ce0f8ae16d",
    "id": "Wwf4uKjTIYm2",
    "outputId": "9fac48b8-49af-44ff-f06b-d4b7440110b7"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           deskripsi\n0  inggris larang impor produk kesehatan xinjiang...\n1  tempoco ramadan tak hanya indonesia negara ini...\n2  tumbuh persen amar bank salurkan pinjaman rp  ...\n3  kiat menjaga kesehatan hati reporter bisniscom...\n4  pesenam kelahiran new york tetap tak boleh ber...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deskripsi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>inggris larang impor produk kesehatan xinjiang...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tempoco ramadan tak hanya indonesia negara ini...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tumbuh persen amar bank salurkan pinjaman rp  ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>kiat menjaga kesehatan hati reporter bisniscom...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pesenam kelahiran new york tetap tak boleh ber...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "_uuid": "bd907b93c352fce35e8e6a886c01e9cf344ce455",
    "id": "FxegmHWtI3eJ",
    "outputId": "d0d3d76b-982e-422a-cdab-e05dfa72c536"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           deskripsi\n0  inggris larang impor produk kesehatan xinjiang...\n1  tempoco ramadan tak hanya indonesia negara ini...\n2  tumbuh persen amar bank salurkan pinjaman rp  ...\n3  kiat menjaga kesehatan hati reporter bisniscom...\n4  pesenam kelahiran new york tetap tak boleh ber...\n5  bursa transfer liga rans cilegon fc kembali da...\n6  siaran tv analog dihentikan april kominfo taha...\n7  kualifikasi formula emilia romagna verstappen ...\n8  bursa transfer liga psis semarang dapatkan pem...\n9  gempa terkini di sulawesi tengah giliran morow...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deskripsi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>inggris larang impor produk kesehatan xinjiang...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tempoco ramadan tak hanya indonesia negara ini...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tumbuh persen amar bank salurkan pinjaman rp  ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>kiat menjaga kesehatan hati reporter bisniscom...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pesenam kelahiran new york tetap tak boleh ber...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bursa transfer liga rans cilegon fc kembali da...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>siaran tv analog dihentikan april kominfo taha...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>kualifikasi formula emilia romagna verstappen ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>bursa transfer liga psis semarang dapatkan pem...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>gempa terkini di sulawesi tengah giliran morow...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bad03b7f6b2b69cd4554ec9e7681b430dfe4e10"
   },
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi yang bermanfaat untuk memperkecil nilai kompleksitas dalam pemrosesan term-document matrix. SVD merupakan teorema aljabar linier yang menyebutkan bahwa persegi panjang dari term-document matrix dapat dipecah/didekomposisikan menjadi tiga matriks, yaitu :\n",
    "\n",
    "    - Matriks ortogonal U\n",
    "    - Matriks diagonal S\n",
    "    - Transpose dari matriks ortogonal V\n",
    "Yang dirumuskan dengan :\n",
    "$A_{mn} = U_{mm} \\times S_{mn} \\times V_{nn}^{T}$\n",
    "\n",
    "$A_{mn}$ = matriks awal\n",
    "\n",
    "$U_{mm}$ = matriks ortogonal U\n",
    "\n",
    "$S_{mn}$ = matriks diagonal s\n",
    "\n",
    "$V_{nn}^{T}$ = transpose matriks ortogonal v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "_uuid": "ac483df5e69eeb2b4b71af956c6987896eda7eb6",
    "id": "9xuGfFdtqTp5"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "vect=TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(df['deskripsi'])\n",
    "lsa_top=lsa_model.fit_transform(vect_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "_uuid": "0a8f4620d3c71db1275e4b5662bf4ab32c598266",
    "id": "KdaVWPC6xmq-",
    "outputId": "d23a23e6-22bb-41dc-8b6a-508dd20a67c2",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.44246597 -0.05170996 -0.20169801 -0.28317528 -0.00783806  0.04909038\n  -0.04062387  0.25735175 -0.23806626 -0.13277994]\n [ 0.43834268 -0.01483974 -0.26760943  0.04184514 -0.25218383 -0.36419608\n   0.14307254 -0.10902938  0.16825659 -0.00868517]\n [ 0.24292104  0.08217825 -0.28315927  0.52872096 -0.02739613  0.45285055\n  -0.020461    0.05261666 -0.04663431  0.16005786]\n [ 0.35457706 -0.0551464  -0.21694956 -0.28766721  0.14532066  0.21002176\n   0.20902009  0.22368978 -0.19391787 -0.26777921]\n [ 0.30787958  0.02895696 -0.21166957 -0.25600217 -0.12710546  0.132557\n  -0.45101321 -0.2384806  -0.13992888  0.00296068]\n [ 0.26639684  0.69285617  0.16963238  0.09609313  0.10021745 -0.01081079\n   0.1316889  -0.07574922 -0.17049884 -0.14253969]\n [ 0.34882707 -0.11409444  0.11444984  0.11679567 -0.1365982   0.02344264\n  -0.27112742  0.37011553 -0.20367929 -0.29926827]\n [ 0.39792754  0.09482178  0.09811501 -0.2101082  -0.16634977  0.20160601\n  -0.03655259 -0.33012514  0.24504065  0.11341437]\n [ 0.28635911  0.68978099  0.20718492  0.02884706  0.08135909 -0.06611009\n   0.09109246 -0.090853   -0.10445768 -0.08070228]\n [ 0.33938486 -0.2124202   0.26190795  0.09752455  0.37072515  0.03091748\n   0.01349875 -0.06488705 -0.26524706  0.44019182]\n [ 0.47111712 -0.06987505 -0.10840237 -0.06431857 -0.09663164 -0.13410123\n  -0.01320776 -0.03486384 -0.15672541 -0.00149571]\n [ 0.26326653 -0.02395139 -0.09738389 -0.15800685  0.23902394  0.44539156\n   0.38747412  0.13050914  0.47072877 -0.06315503]\n [ 0.35286443 -0.11333694 -0.05317334  0.25874906  0.26273245 -0.36316188\n  -0.1207123  -0.07051728  0.38947425 -0.42846294]\n [ 0.30201696  0.03585865 -0.34179375  0.53715758 -0.16048793  0.12247177\n  -0.10815128 -0.0347858   0.0390011   0.05613284]\n [ 0.49730071 -0.30426734  0.48332059  0.12610493 -0.05204604  0.03286607\n   0.12897137 -0.04350953 -0.07359893 -0.0647296 ]\n [ 0.22570571  0.28344491  0.17266675 -0.08521283 -0.13288108 -0.14404727\n  -0.24050054  0.63582419  0.34520292  0.38670758]\n [ 0.33414429 -0.07933979 -0.05254161  0.01369865  0.65796915 -0.13144133\n  -0.25797446 -0.03465512  0.03656343  0.12948484]\n [ 0.45098337  0.01529645 -0.03343537 -0.30031543 -0.11062293  0.04753273\n  -0.17283491 -0.25280043  0.16041879  0.15184777]\n [ 0.3819687  -0.24634975  0.51487718  0.11905172 -0.28003863  0.06519414\n   0.1429403  -0.02609714  0.0478376  -0.05216841]\n [ 0.30614238 -0.06024969 -0.32535298 -0.01465236 -0.07338366 -0.37113611\n   0.4818435   0.0550941  -0.16269096  0.30481745]]\n(20, 10)\n"
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)  # (no_of_doc*no_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "_uuid": "2a259e5f3b8c11118368439eea8de0faa8b7a1c2",
    "id": "d4dvIGIGxtAI",
    "outputId": "58590025-b601-4160-956f-49247c3e064e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Document 0 :\nTopic  0  :  44.246596604769756\nTopic  1  :  -5.170995919999537\nTopic  2  :  -20.16980149986444\nTopic  3  :  -28.317527866218434\nTopic  4  :  -0.7838064817234477\nTopic  5  :  4.909037547612574\nTopic  6  :  -4.062387100114588\nTopic  7  :  25.735174955352846\nTopic  8  :  -23.806625838609815\nTopic  9  :  -13.277994386615799\n"
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2153bcbf45e7623efc213bb94f0dbba377ff34c3"
   },
   "source": [
    "Similalry for other documents we can do this. However note that values dont add to 1 as in LSA it is not probabiltiy of a topic in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "_uuid": "61bb8ff8b4b6cf0cd2fcde7890f3a3ebb8980452",
    "id": "AI2kOuwitOGp",
    "outputId": "093e45e0-2141-4504-b0fe-93e7656731a5",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10, 1000)\n[[ 0.0073628   0.040238    0.02293696 ...  0.02613302  0.01697828\n   0.06184327]\n [-0.00940885 -0.0001586  -0.02780617 ... -0.02786706 -0.00498949\n   0.00415615]\n [ 0.02084096 -0.01926479  0.0468113  ...  0.03530444 -0.00820356\n  -0.00962801]\n ...\n [-0.00136181 -0.0485069  -0.00543261 ... -0.00965279 -0.00340132\n  -0.09384638]\n [ 0.00253737  0.02409803 -0.00934085 ... -0.02668339 -0.01554184\n   0.06053206]\n [-0.00282135  0.04958969 -0.00837633 ...  0.04445687 -0.00015123\n   0.05842172]]\n"
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6f834000fa7b2f294694572fda0bcf56d25badd6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a289f6e017d0c573d20ed386582df71437fab5cc"
   },
   "source": [
    "#### Now e can get a list of the important words for each of the 10 topics as shown. For simplicity here I have shown 10 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "_uuid": "c9860197f6d11e5583629cca322e19a93d4c0350",
    "id": "GMMmSTbQqfdz",
    "outputId": "4a5765e5-ffc9-410f-ce48-1b422d348ee3",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic 0: \ndi dan yang untuk hujan akan ini dengan pada dari \n\nTopic 1: \npemain liga psis fc musim fajar bursa rans baru cilegon \n\nTopic 2: \nhujan pukul bmkg gempa pemain laut hingga liga petir berawan \n\nTopic 3: \nbank amar bpk pinjaman jalur yogyakarta ketua rp mandiri mudik \n\nTopic 4: \npengajar gempa platform siswa bisa yogyakarta jalur mahasiswa aplikasi baterai \n\nTopic 5: \nbaterai bank amar pinjaman listrik motor hati yamaha rp daya \n\nTopic 6: \nbihalal halal baterai listrik motor hati edaran ppkm yamaha idul \n\nTopic 7: \nvs liga inggris hati tv jadwal juara siaran analog kominfo \n\nTopic 8: \nbaterai jalur vs listrik motor yamaha yogyakarta mudik daya swap \n\nTopic 9: \ngempa vs bihalal halal dirasakan guncangan morowali terkini bank juara \n\n"
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "15fc699124fc73b2974081fd6e85752cbd9cd440"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2898911f7af52671923cf6bbc8cab6bc7871aa59",
    "id": "gmMeb4METhm2"
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd861ec68f8272be78421f8bd76b98793009e836"
   },
   "source": [
    "LDA is the most popular technique.**The topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.**\n",
    "\n",
    "**To understand the maths it seems as if knowledge of Dirichlet distribution (distribution of distributions) is required which is quite intricate and left fior now.**\n",
    "\n",
    "To get an inituitive explanation of LDA checkout these blogs: [this](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)  ,  [this](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)  ,[this](https://en.wikipedia.org/wiki/Topic_model)  ,  [this kernel on Kaggle](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial)  ,  [this](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "_uuid": "b21a29802cac8f469c65df49ba0ef04f592f3202",
    "id": "yPJFHVxxTiwh"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1) \n",
    "# n_components is the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "_uuid": "8c0169b02a356e8cf8583218437d23f549c1e3da",
    "id": "aeUPTUUIazvB"
   },
   "outputs": [],
   "source": [
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "_uuid": "77425315af5c000fa30caefd6dcebb70c2970cfa",
    "id": "h8LzsFixdkBt",
    "outputId": "4ffc31da-a283-4cc6-a7e8-67c7ca8e5224",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20, 10)\n[[0.01032322 0.01032308 0.90709126 0.01032321 0.01032319 0.01032308\n  0.01032351 0.0103232  0.01032302 0.01032323]\n [0.01072433 0.01072439 0.01072439 0.01072452 0.01072441 0.01072434\n  0.90348067 0.01072431 0.01072434 0.0107243 ]\n [0.01372216 0.01372214 0.01372216 0.01372221 0.87650016 0.01372218\n  0.01372206 0.01372236 0.01372229 0.01372228]\n [0.01115603 0.01115536 0.01115543 0.01115535 0.01115543 0.01115537\n  0.01115539 0.01115535 0.89960094 0.01115536]\n [0.91078083 0.009913   0.00991311 0.00991299 0.00991311 0.00991311\n  0.0099144  0.0099131  0.00991308 0.00991326]\n [0.01299454 0.01299356 0.01299364 0.01299364 0.01299364 0.01299366\n  0.01299356 0.01299359 0.01299366 0.88305651]\n [0.01193168 0.01193168 0.0119314  0.0119315  0.01193141 0.01193141\n  0.01193145 0.01193136 0.89261651 0.01193161]\n [0.0088771  0.00887703 0.0088771  0.00887701 0.00887713 0.00887704\n  0.0088771  0.9201061  0.0088771  0.00887729]\n [0.01266568 0.01266549 0.01266568 0.01266567 0.01266566 0.01266573\n  0.01266565 0.01266568 0.88600894 0.01266583]\n [0.01280819 0.88472676 0.0128081  0.01280817 0.01280806 0.01280811\n  0.01280814 0.01280796 0.01280815 0.01280834]\n [0.00838822 0.92450624 0.00838816 0.00838824 0.0083882  0.00838818\n  0.00838818 0.0083882  0.00838817 0.00838821]\n [0.01270717 0.01270703 0.01270692 0.01270689 0.0127069  0.01270688\n  0.01270697 0.01270689 0.0127071  0.88563724]\n [0.0109299  0.01093003 0.01092991 0.90163113 0.01092981 0.01092984\n  0.01092984 0.01092986 0.01092981 0.01092987]\n [0.01269457 0.0126946  0.01269455 0.88574844 0.01269492 0.01269449\n  0.01269448 0.01269462 0.01269471 0.01269462]\n [0.01035195 0.01035194 0.01035201 0.01035204 0.01035201 0.01035207\n  0.01035193 0.01035193 0.0103519  0.90683221]\n [0.01275627 0.01275629 0.01275636 0.01275633 0.01275603 0.0127564\n  0.01275623 0.01275636 0.01275624 0.88519349]\n [0.90604786 0.01043906 0.01043909 0.01043923 0.01043906 0.01043921\n  0.01043913 0.01043909 0.01043914 0.01043911]\n [0.90979083 0.0100242  0.0100231  0.01002311 0.01002316 0.01002305\n  0.01002313 0.01002315 0.01002313 0.01002315]\n [0.01251804 0.01251803 0.01251802 0.01251799 0.88733773 0.01251803\n  0.01251797 0.01251797 0.01251805 0.01251819]\n [0.01283432 0.88449084 0.01283416 0.01283434 0.01283434 0.01283424\n  0.01283492 0.01283425 0.01283441 0.01283419]]\n"
    }
   ],
   "source": [
    "print(lda_top.shape)  # (no_of_doc,no_of_topics)\n",
    "print(lda_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "_uuid": "2fc450b8316e62d050eb702b65eddbb1e898248f",
    "id": "YNgP0S0eeVuO",
    "outputId": "a84c1854-e3f2-454f-ed91-f4776841652a",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0\n"
    }
   ],
   "source": [
    "sum=0\n",
    "for i in lda_top[0]:\n",
    "  sum=sum+i\n",
    "print(sum)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f6d8a55e8a513a9af61ae480eae5a0e2135f182"
   },
   "source": [
    "#### Note that the values in a particular row adds to 1. This is beacuse each value denotes the % of contribution of the corressponding topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "_uuid": "d896b8fcf2c398367de4e7b7142e144fb4944f2d",
    "id": "Z6WmZLp3ehbY",
    "outputId": "002dfe31-7d3f-4186-8930-a12e175d6be3",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Document 0: \nTopic  0 :  1.0323224683850167 %\nTopic  1 :  1.032307504454671 %\nTopic  2 :  90.70912634927686 %\nTopic  3 :  1.0323205782698202 %\nTopic  4 :  1.0323194720162474 %\nTopic  5 :  1.0323076514533123 %\nTopic  6 :  1.0323511454775984 %\nTopic  7 :  1.0323200383865658 %\nTopic  8 :  1.0323017599576698 %\nTopic  9 :  1.0323230323222345 %\n"
    }
   ],
   "source": [
    "# composition of doc 0 for eg\n",
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d24927b415ef64338a4b99e81c9cafd5225dfae5",
    "id": "NVgtw_L_g8N2"
   },
   "source": [
    "#### As we can see Topic 7 & 8 are dominantly present in document 0.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "_uuid": "3ee132a423ca0bfe3107fa3dd2652d6c33d5bc4d",
    "id": "6OFxmyReiZIU",
    "outputId": "9dc5bcef-32cb-482d-c62c-e6644f12cc6e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.8703105  0.85434455 0.81044014 ... 0.93537146 0.75631191 0.91473369]\n [0.91109682 0.88051115 0.62901973 ... 0.84263822 0.87510869 1.05082216]\n [0.68888338 0.82668677 0.88243706 ... 0.9336661  0.77714279 0.80835138]\n ...\n [0.75684535 0.91820279 0.85334298 ... 0.79899823 0.6764335  0.81364785]\n [0.9376118  0.76222521 0.80687708 ... 0.88485635 0.79560354 0.81234631]\n [0.75327363 0.76478809 0.96002918 ... 0.87954728 0.69845729 0.72556003]]\n(10, 1000)\n"
    }
   ],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)  # (no_of_topics*no_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bf169b07cefbe2cd8e36bc1e2bd65ca7c398374",
    "id": "NoEr9qt1jgsM"
   },
   "source": [
    "#### Most important words for a topic. (say 10 this time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "_uuid": "5c13c5563657ac5103a33615272320065542fb40",
    "id": "hKJHM0C-l0an",
    "outputId": "09fc0532-8444-42aa-85fa-a986ed72ee04",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic 0: \nbila siswa tidak mendapat kesempatan resmi ujar switch tahun balapan \n\nTopic 1: \npenggemar lalu makanmakan ukraina guncangan lihat zendaya klimatologi diluncurkan ppkm \n\nTopic 2: \nbelakang setiap ritmik ukrainarusia prancis bangkit sprint servicedengan pernah pendidikan \n\nTopic 3: \ntepatnya diprediksi nomor tingkat hidup perjalanan laga mengisi tak jalur \n\nTopic 4: \nolahan lap abang kawasan iii pertumbuhan mirip poin satu kelapa \n\nTopic 5: \ntumbuh diluncurkan takjil sendiri kontributor setiawan tingkat alternatif katanya mengikuti \n\nTopic 6: \nramadan menentukan mempunyai memiliki risiko layanan platform tapi guncangan ujiajacommenerapkan \n\nTopic 7: \nbisa kliktempoco kerawanan sepanjang naik lama kasino pemerintah insiden sosial \n\nTopic 8: \ntampak keseluruhan ucapnya mendukung narendra rekam virtual diguyur membawa terkait \n\nTopic 9: \nterdapat baterai balik tentunya bidang empat kantor selasa ahad laut \n\n"
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "854f0eb88cf78923db8137a2f41581eb9bbd5592"
   },
   "source": [
    "#### To better visualize words in a topic we can see the word cloud. For each topic top 50 words are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "_uuid": "5221ae4de021107ef757b87d45053460e50582c8",
    "id": "_ac73PUhmZmn"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3172/1754887305.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Generate a word cloud image for given topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_word_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mimp_words_topic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mcomp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "# Generate a word cloud image for given topic\n",
    "def draw_word_cloud(index):\n",
    "  imp_words_topic=\"\"\n",
    "  comp=lda_model.components_[index]\n",
    "  vocab_comp = zip(vocab, comp)\n",
    "  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:50]\n",
    "  for word in sorted_words:\n",
    "    imp_words_topic=imp_words_topic+\" \"+word[0]\n",
    "\n",
    "  wordcloud = WordCloud(width=600, height=400).generate(imp_words_topic)\n",
    "  plt.figure( figsize=(5,5))\n",
    "  plt.imshow(wordcloud)\n",
    "  plt.axis(\"off\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "_uuid": "e5246db8bb1b751a1f3b96bb65d4da9e34dfc2ba",
    "id": "-tD4nZdRqnAk",
    "outputId": "8257c45d-92b5-4472-da45-864aaf803915"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'draw_word_cloud' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3172/786484936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# topic 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdraw_word_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_word_cloud' is not defined"
     ]
    }
   ],
   "source": [
    "# topic 0\n",
    "draw_word_cloud(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "_uuid": "74872c17b12680171090690f62ce77019e03ef52",
    "id": "Jz_zGbSws1ns",
    "outputId": "5289432c-61fb-48ec-df58-d28d383ea08d"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'draw_word_cloud' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3172/2024154827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# topic 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdraw_word_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_word_cloud' is not defined"
     ]
    }
   ],
   "source": [
    "# topic 1\n",
    "draw_word_cloud(1)  # ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python39564bit55ec6bc4e91143519769c30b44c5d730"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}